%% ============================================================================
%% REFERENCE ARCHITECTURES
%% ============================================================================
\section{Reference Architectures}
\label{sec:architectures}

WaveDL's registry pattern is designed to support a wide variety of neural network architectures. The framework ships with baseline implementations and is architected for seamless integration of popular pre-trained models from the computer vision literature. This section describes the included reference architectures and the extensibility mechanisms for adding new models.

\subsection{Included Models}

The current release provides two built-in architectures:
\begin{enumerate}
    \item \textbf{SimpleCNN} (\texttt{simplecnn}): A lightweight convolutional neural network suitable for baseline experiments and rapid prototyping. It serves as a template for custom model development.
    \item \textbf{Additional architectures} (planned): The modular design supports integration of established architectures including ResNet~\cite{resnet2016}, EfficientNet~\cite{efficientnet2019}, and Vision Transformers (ViT)~\cite{vit2021}. Users can add these by implementing the \texttt{BaseModel} interface and registering via the \texttt{@register\_model} decorator.
\end{enumerate}

\subsection{SimpleCNN Architecture}

The \texttt{SimpleCNN} model provides a straightforward encoder-decoder architecture for regression on 2D inputs. It is intentionally minimal to serve as a baseline and starting point for modifications.

\begin{table*}[htbp]
\centering
\caption{SimpleCNN Layer Specifications}
\label{tab:simplecnn}
\begin{tabular}{llcccc}
\toprule
\textbf{Block} & \textbf{Layer Type} & \textbf{Ch.} & \textbf{Kernel} & \textbf{Stride} & \textbf{Output} \\
\midrule
Input & -- & 1 & -- & -- & $H \times W$ \\
1 & Conv2D + GroupNorm + LeakyReLU & 16 & 3$\times$3 & 1 & $H \times W$ \\
1 & MaxPool2D & 16 & 2$\times$2 & 2 & $H/2 \times W/2$ \\
2 & Conv2D + GroupNorm + LeakyReLU & 32 & 3$\times$3 & 1 & $H/2 \times W/2$ \\
2 & MaxPool2D & 32 & 2$\times$2 & 2 & $H/4 \times W/4$ \\
3 & Conv2D + GroupNorm + LeakyReLU & 64 & 3$\times$3 & 1 & $H/4 \times W/4$ \\
3 & MaxPool2D & 64 & 2$\times$2 & 2 & $H/8 \times W/8$ \\
4 & Conv2D + GroupNorm + LeakyReLU & 128 & 3$\times$3 & 1 & $H/8 \times W/8$ \\
4 & MaxPool2D & 128 & 2$\times$2 & 2 & $H/16 \times W/16$ \\
5 & Conv2D + GroupNorm + LeakyReLU & 256 & 3$\times$3 & 1 & $H/16 \times W/16$ \\
5 & AdaptiveAvgPool2D & 256 & -- & -- & 4$\times$4 \\
Head & Flatten & 4096 & -- & -- & -- \\
Head & FC + LayerNorm + LeakyReLU + Dropout & 512 & -- & -- & -- \\
Head & FC + LayerNorm + LeakyReLU + Dropout & 256 & -- & -- & -- \\
Head & FC (Output) & $K$ & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table*}

\textbf{Design Rationale}:
\begin{itemize}
    \item \textbf{GroupNorm} instead of BatchNorm ensures stable training with small per-GPU batch sizes in distributed settings~\cite{wu2018}.
    \item \textbf{LeakyReLU} (slope=0.1) prevents dead neurons during training.
    \item \textbf{Dropout} ($p=0.5$) in the fully-connected layers provides regularization against overfitting.
\end{itemize}

% Figure: SimpleCNN architecture
\begin{figure*}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{images/fig4_simplecnn_architecture.png}
\caption{SimpleCNN architecture for regression. The encoder progressively reduces spatial dimensions through 5 convolutional blocks with max pooling, followed by adaptive average pooling and a 3-layer fully-connected regression head.}
\label{fig:simplecnn}
\end{figure*}

\subsection{Extensibility: Adding New Architectures}

The registry pattern enables users to integrate any PyTorch model without modifying the training infrastructure. To add a new architecture:

\textbf{Example: Integrating ResNet-18 for Regression}
\begin{lstlisting}[caption={Integrating ResNet-18 for regression}]
# models/resnet_regressor.py
import torch.nn as nn
from torchvision.models import resnet18, ResNet18_Weights
from models.base import BaseModel
from models.registry import register_model

@register_model("resnet18")
class ResNet18Regressor(BaseModel):
    """ResNet-18 adapted for multi-output regression."""
    
    def __init__(self, in_shape, out_size, pretrained=True, **kwargs):
        super().__init__(in_shape, out_size)
        
        # Load pre-trained backbone
        weights = ResNet18_Weights.DEFAULT if pretrained else None
        backbone = resnet18(weights=weights)
        
        # Modify input layer for single-channel images
        backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
        
        # Replace classification head with regression head
        backbone.fc = nn.Sequential(
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.LeakyReLU(0.1),
            nn.Dropout(0.5),
            nn.Linear(256, out_size)
        )
        
        self.model = backbone
    
    def forward(self, x):
        return self.model(x)
\end{lstlisting}

After registration, the model is immediately available:
\begin{verbatim}
accelerate launch train.py --model resnet18 --data_path train_data.npz
\end{verbatim}

This same pattern applies to EfficientNet, Vision Transformers, or any custom architecture.

\subsection{Training Loop Overview}

The training procedure is model-agnostic and follows standard supervised learning with HPC-specific adaptations:

\textbf{Algorithm 2: WaveDL Training Loop}
\begin{lstlisting}[style=pseudocode,caption={WaveDL Training Loop}]
INPUT: Dataset D, Model N_theta, Hyperparameters (lr, patience, epochs, ...)
OUTPUT: Trained model weights theta*

1.  Initialize optimizer <- AdamW(theta, lr, weight_decay)
2.  Initialize scheduler <- ReduceLROnPlateau(optimizer, factor=0.5, patience=5)
3.  best_loss <- infinity, patience_ctr <- 0

4.  FOR epoch = 1 TO epochs DO:
5.      // --- Training Phase ---
6.      N_theta.train()
7.      FOR each batch (x, y) in train_loader DO:
8.          y_hat <- N_theta(x)
9.          loss <- MSE(y_hat, y)
10.         accelerator.backward(loss)
11.         IF accelerator.sync_gradients:
12.             clip_grad_norm_(theta, max_norm=1.0)
13.         optimizer.step()
14.         optimizer.zero_grad()
15.     END FOR
        
16.     // --- Validation Phase ---
17.     N_theta.eval()
18.     val_loss_sum, val_count <- 0, 0
19.     WITH torch.no_grad():
20.         FOR each batch (x, y) in val_loader DO:
21.             y_hat <- N_theta(x)
22.             val_loss_sum += MSE(y_hat, y) * batch_size
23.             val_count += batch_size
24.             Compute physical MAE per target
25.         END FOR
26.     END WITH
        
27.     // --- DDP-Safe Aggregation ---
28.     val_loss <- all_reduce(val_loss_sum) / all_reduce(val_count)
        
29.     // --- Checkpointing ---
30.     IF val_loss < best_loss AND is_main_process:
31.         save_checkpoint(theta, optimizer, scheduler, epoch)
32.         best_loss <- val_loss
33.         patience_ctr <- 0
34.     ELSE:
35.         patience_ctr += 1
36.     END IF
        
37.     // --- DDP-Safe Early Stopping ---
38.     should_stop <- (patience_ctr >= patience) IF is_main_process ELSE False
39.     IF broadcast_early_stop(should_stop):
40.         BREAK
41.     END IF
        
42.     scheduler.step(val_loss)
43.  END FOR

44.  RETURN theta*
\end{lstlisting}

\subsection{Hyperparameter Selection Rationale}

\begin{table*}[htbp]
\centering
\caption{Default hyperparameters and rationale}
\label{tab:hyperparams}
\begin{tabular}{lll}
\toprule
\textbf{Hyperparameter} & \textbf{Default} & \textbf{Rationale} \\
\midrule
Learning Rate & $10^{-3}$ & Standard for AdamW; aggressive enough for fast convergence \\
Weight Decay & $10^{-4}$ & Mild L2 regularization; prevents overfitting without underfitting \\
Batch Size & 128 & Balances GPU memory utilization ($\sim$8GB) and gradient noise \\
Patience & 20 & Allows temporary plateaus while preventing overtraining \\
Dropout & 0.5 & Aggressive regularization; physics data is often redundant \\
Gradient Clip & 1.0 & Prevents gradient explosion in early epochs; critical for stability \\
\bottomrule
\end{tabular}
\end{table*}

% Figure: Training pipeline
\begin{figure*}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{images/fig5_training_pipeline.png}
\caption{WaveDL training pipeline flowchart. The pipeline includes memory-mapped data loading, DDP-safe gradient synchronization, automatic checkpointing, and synchronized early stopping to ensure robust multi-GPU training.}
\label{fig:training_pipeline}
\end{figure*}

%% ============================================================================
%% EXPERIMENTAL VALIDATION
%% ============================================================================
\section{Experimental Validation}
\label{sec:experiments}

We validate WaveDL's efficacy through a comprehensive case study on Lamb wave dispersion curve inversion. This problem is representative of a broad class of inverse problems in acoustics and geophysics where the goal is to recover physical parameters from spectral signatures.

\subsection{Dataset Generation Protocol}

The training database was generated using the analytical Rayleigh-Lamb dispersion equations for an isotropic plate. For a plate of thickness $h$, longitudinal velocity $c_L$, and shear velocity $c_T$, the phase velocity $c_p$ of symmetric ($S$) and antisymmetric ($A$) modes satisfies:

% Figure: Dispersion curves
\begin{figure*}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{images/fig6_dispersion_curves.png}
\caption{Example Lamb wave dispersion curves for an aluminum plate showing symmetric (S0, S1) and antisymmetric (A0, A1) modes. These curves form the basis of the synthetic training data used for network validation.}
\label{fig:dispersion}
\end{figure*}

\begin{align}
\frac{\tan(qh)}{\tan(ph)} &= -\frac{4k^2pq}{(k^2-q^2)^2} \quad \text{(Symmetric modes)} \\
\frac{\tan(qh)}{\tan(ph)} &= -\frac{(k^2-q^2)^2}{4k^2pq} \quad \text{(Antisymmetric modes)}
\end{align}
where $k = \omega/c_p$ is the wavenumber, $p^2 = (\omega/c_L)^2 - k^2$, and $q^2 = (\omega/c_T)^2 - k^2$.

We generated \textbf{100,000 samples} by uniformly sampling the material parameter space:
\begin{itemize}
    \item Thickness $h$: $1.0 - 10.0$~mm
    \item Shear Velocity $c_T$: $3000 - 3250$~m/s
    \item Longitudinal Velocity $c_L$: Fixed ratio $c_L = 2c_T$ (assuming constant Poisson's ratio)
\end{itemize}

The resulting dispersion curves were rasterized into $500 \times 500$ binary images (1 if a mode exists at $(f, k)$, 0 otherwise), simulating experimentally obtained frequency-wavenumber spectra.

\subsection{Training Protocol}

The network was trained using the WaveDL \texttt{run\_training.sh} pipeline on a cluster node with 4 NVIDIA V100 GPUs. The hyperparameters were selected based on standard practices for ResNet-style architectures.

\begin{table*}[htbp]
\centering
\caption{Hyperparameter Configuration}
\label{tab:training_config}
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Justification} \\
\midrule
Optimizer & AdamW & Standard for modern CNNs; handles weight decay effectively \\
Learning Rate & $1 \times 10^{-3}$ & Initial rate, decayed using ReduceLROnPlateau \\
Batch Size & 128 (Total) & 32 per GPU, balanced for V100 memory and convergence \\
Weight Decay & $1 \times 10^{-4}$ & L2 regularization to prevent overfitting \\
Loss Function & MSE & Mean Squared Error on standardized targets \\
Precision & BF16 & Mixed precision (Brain Float 16) for 30\% training speedup \\
Patience & 20 epochs & Strict early stopping to prevent overtraining \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Evaluation Metrics}

To quantify performance rigorously, we employ three complementary metrics:
\begin{enumerate}
    \item \textbf{Physical MAE} ($\epsilon_{phys}$): The average absolute deviation in physical units.
    \begin{equation}
    \epsilon_{phys} = \frac{1}{N} \sum |\hat{y} - y|
    \end{equation}
    
    \item \textbf{Relative Error} ($\epsilon_{rel}$): Error normalized by the true value, useful for comparing performance across parameters with different magnitudes.
    \begin{equation}
    \epsilon_{rel} = \frac{1}{N} \sum \left| \frac{\hat{y} - y}{y} \right| \times 100\%
    \end{equation}
    
    \item \textbf{Pearson Correlation Coefficient} ($\rho$): Measures the linear correlation between predictions and ground truth.
    \begin{equation}
    \rho = \frac{\text{cov}(\hat{y}, y)}{\sigma_{\hat{y}} \sigma_y}
    \end{equation}
\end{enumerate}

\subsection{Results and Analysis}

The model converged after 85 epochs (approx. 2.5 hours wall-clock time). Table~\ref{tab:results} summarizes the performance on the held-out test set ($N_{test}=10,000$).

\begin{table*}[htbp]
\centering
\caption{Inversion Accuracy on Synthetic Test Set}
\label{tab:results}
\begin{tabular}{lcccc}
\toprule
\textbf{Parameter} & \textbf{MAE} & \textbf{Rel. Error} & \textbf{Pearson $\rho$} & \textbf{Tolerance} \\
\midrule
Thickness ($h$) & 0.042~mm & 0.76\% & $>$0.999 & $\pm$0.1~mm \\
Velocity ($c_T$) & 3.12~m/s & 0.10\% & $>$0.999 & $\pm$10~m/s \\
\bottomrule
\end{tabular}
\end{table*}

\textbf{Analysis of Convergence}: The rapid convergence (sub-100 epochs) suggests that the SimpleCNN architecture successfully captures the underlying physics governing the dispersion curves. The high Pearson correlation ($>0.999$) indicates that the model has learned the global trend of the inverse mapping $f^{-1}$, rather than simply memorizing the training set. The error margins ($0.04$~mm) are well within the typical experimental uncertainty of ultrasonic transducers ($\sim$0.1~mm), suggesting the model is ``super-resolving'' the physical properties relative to standard experimental limits.

% Figure: Training convergence
\begin{figure*}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{images/fig13_training_convergence.png}
\caption{Training convergence curves showing rapid loss reduction and learning rate scheduling. The model converges within 100 epochs with early stopping preventing overfitting.}
\label{fig:convergence}
\end{figure*}

\textbf{Performance Scaling}: We observed a near-linear speedup in training throughput with increasing GPU count. Comparing 1-GPU vs. 4-GPU training:
\begin{itemize}
    \item \textbf{1 GPU}: 140 samples/sec
    \item \textbf{4 GPUs}: 520 samples/sec (Efficiency $\approx 93\%$)
\end{itemize}

% Figure: GPU scaling
\begin{figure*}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{images/fig8_gpu_scaling.png}
\caption{Multi-GPU scaling performance. Training throughput scales near-linearly with GPU count, achieving 93\% efficiency at 4 GPUs, demonstrating that the MemmapDataset implementation successfully removes I/O bottlenecks.}
\label{fig:gpu_scaling}
\end{figure*}

% Figure: Prediction results
\begin{figure*}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{images/fig9_prediction_results.png}
\caption{Prediction accuracy on the test dataset ($N=10,000$). Both thickness and velocity predictions show tight correlation with ground truth ($R^2 > 0.999$), with errors well within engineering tolerances.}
\label{fig:predictions}
\end{figure*}

This confirms that the \texttt{MemmapDataset} implementation successfully removes I/O bottlenecks, allowing the system to be compute-bound even with high-throughput distributed training.
