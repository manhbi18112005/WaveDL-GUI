%% ============================================================================
%% DISCUSSION
%% ============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Computational Complexity Analysis}

\textbf{Space Complexity}: The memory-mapped data formulation reduces the Random Access Memory requirement from $O(N \cdot H \cdot W)$ to $O(B \cdot H \cdot W)$, where $B$ is the batch size. This $O(1)$ scaling with respect to dataset size $N$ is the critical enabler for training on terabyte-scale simulation databases. In practice, we observed that training on a 100~GB dataset required only $\sim$2~GB of RAM per process, independent of the total dataset size.

\textbf{Time Complexity}: The DDP synchronization introduces a communication overhead of $O(\log P)$ per batch, where $P$ is the number of GPUs, due to the tree-structured all-reduce algorithm. Empirical results show near-linear scaling efficiency ($>90\%$) up to 8 GPUs, indicating that the gradient computation dominates the inter-node communication cost. Beyond 8 GPUs, network bandwidth may become the limiting factor, particularly on commodity Ethernet (vs. InfiniBand).

\textbf{Model Complexity}: SimpleCNN contains approximately 1.5 million trainable parameters, corresponding to $\sim$6~MB of storage in float32. This is intentionally lightweight compared to ImageNet-scale models (ResNet-50: 25M parameters) because dispersion curve regression is a fundamentally simpler task than 1000-class classification. The parameter efficiency enables:
\begin{itemize}
    \item Faster training (fewer gradient computations)
    \item Lower memory footprint per GPU
    \item Reduced risk of overfitting on physics-constrained data
\end{itemize}

\subsection{Advantages and Broader Impact}

WaveDL standardizes the ad-hoc scripts often used in physical DL research into a structured, reproducible framework. By abstracting the complexities of distributed computing and data management, it reduces the ``time-to-science'' for researchers. Specific advantages include:

\begin{enumerate}
    \item \textbf{Reproducibility}: Comprehensive logging, deterministic seeding, and checkpoint/resume functionality ensure experiments can be exactly reproduced. This addresses a major crisis in machine learning research where many published results cannot be replicated.
    
    \item \textbf{Fair Benchmarking}: The strict separation of modeling (registry) and infrastructure (training loop) ensures that code developed with WaveDL enables fair, apples-to-apples comparison of different network architectures under identical data splits, augmentation, and optimization settings.
    
    \item \textbf{Community Building}: By providing a common framework with clear extension points, WaveDL lowers the barrier for researchers to contribute new architectures, metrics, and datasets. This fosters collaboration and knowledge sharing within the quantitative ultrasonics community.
    
    \item \textbf{Industrial Applicability}: The production-ready features (mixed precision, checkpoint resume, robust error handling) make WaveDL suitable for deployment in industrial NDE pipelines, not just academic research.
\end{enumerate}

\subsection{Benchmarking vs. Physics-Based Solvers}

To contextualize the performance gains, we compare WaveDL's inference time against standard numerical solvers used in NDE: the Transfer Matrix Method (TMM) and the Global Matrix Method (GMM).

\begin{table*}[htbp]
\centering
\caption{Inference Speed Comparison}
\label{tab:speed}
\begin{tabular}{llll}
\toprule
\textbf{Method} & \textbf{Inference Time} & \textbf{Hardware} & \textbf{Notes} \\
\midrule
Transfer Matrix (TMM) & 150--500~ms/point & CPU (single core) & Root-finding dependent \\
Global Matrix (GMM) & 50--200~ms/point & CPU (single core) & Faster for thin plates \\
Finite Element (FEM) & 10--60~s/point & CPU (multi-core) & For complex geometries \\
WaveDL (SimpleCNN) & 0.12~ms/point & GPU (V100) & Batch of 128; amortized \\
\bottomrule
\end{tabular}
\end{table*}

\textbf{Speedup Analysis}:
\begin{equation}
\text{Speedup Factor} \approx \frac{250~\text{ms (TMM)}}{0.12~\text{ms (WaveDL)}} \approx 2000\times
\end{equation}

This three-order-of-magnitude acceleration enables real-time inversion at acquisition rates exceeding 1~kHz, a feat unattainable with traditional iterative optimization. For in-line inspection of pipelines at scanning speeds of 1~m/s with measurement points every 1~mm, this corresponds to processing 1000 points per second---achievable with WaveDL but impossible with TMM.

\subsection{Limitations and Scope}

While WaveDL addresses key challenges in guided wave deep learning, several limitations warrant acknowledgment:

\begin{enumerate}
    \item \textbf{Regression-Only Design}: The current framework is optimized for multi-output regression. Classification tasks (e.g., defect/no-defect) or segmentation (e.g., defect localization maps) would require modifications to loss functions, metrics, and potentially the data pipeline. Adapting WaveDL for classification is straightforward but not included in the current release.
    
    \item \textbf{2D Input Assumption}: The data pipeline assumes 2D image representations. Raw 1D time-domain signals or 3D volumetric data (e.g., from phased array imaging) would require modifications to the \texttt{MemmapDataset} shape handling and model input layers.
    
    \item \textbf{No Automated Hyperparameter Tuning}: WaveDL provides sensible defaults but does not include integrated hyperparameter optimization (e.g., Optuna, Ray Tune). Users must manually explore learning rates, architectures, and regularization settings. Future work could integrate Weights \& Biases Sweeps for automated search.
    
    \item \textbf{Limited Preprocessing}: The framework assumes input data is already in the desired representation (dispersion curves, spectrograms). Signal processing steps (FFT, STFT, wavelet transforms) are left to the user. A future ``transforms'' module could standardize these operations.
    
    \item \textbf{Synthetic Data Bias}: The validation presented uses synthetically generated data from analytical models. Generalization to experimental data with noise, calibration errors, and missing modes requires domain adaptation techniques not currently included.
\end{enumerate}

\subsection{Future Directions}

Based on the current implementation and community feedback, we identify the following development priorities:

\begin{enumerate}
    \item \textbf{Unit Tests and CI/CD}: Adding comprehensive test coverage would enable automated verification of changes, critical for accepting community contributions without regression risks. Integration with GitHub Actions for continuous integration is planned.
    
    \item \textbf{PyPI Distribution}: Packaging WaveDL for installation via \texttt{pip install wavedl} would simplify adoption and version management.
    
    \item \textbf{Physics-Informed Loss Functions}: Incorporating known wave physics (e.g., dispersion curve smoothness, mode ordering constraints) as differentiable regularization terms could improve generalization and reduce data requirements.
    
    \item \textbf{Pre-trained Models and Transfer Learning}: Providing pre-trained weights on standardized benchmark datasets would enable transfer learning, reducing training time for users with limited computational resources.
    
    \item \textbf{Uncertainty Quantification}: Ensemble methods, Monte Carlo dropout, or Bayesian neural networks could provide confidence intervals on predictions---critical for high-stakes industrial applications.
    
    \item \textbf{1D and 3D Extensions}: Generalizing the data pipeline to handle raw time-series (1D) and volumetric (3D) inputs would broaden applicability to diverse NDE scenarios.
\end{enumerate}

%% ============================================================================
%% CONCLUSION
%% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We have presented \textbf{WaveDL}, a robust deep learning framework specifically designed for the inverse characterization of guided waves in non-destructive evaluation applications. By addressing the fundamental engineering challenges that have hindered the transition from proof-of-concept studies to production-ready research tools, WaveDL enables researchers to focus on scientific innovation rather than computational infrastructure.

\textbf{Key Contributions}:
\begin{enumerate}
    \item \textbf{Zero-Copy Memory-Mapped Pipeline}: Enables training on datasets exceeding available RAM ($O(1)$ memory complexity with respect to dataset size), eliminating the data scale barrier.
    
    \item \textbf{DDP-Safe Synchronization}: Prevents early stopping deadlocks and ensures correct metric aggregation across multi-GPU clusters, enabling reliable HPC deployment.
    
    \item \textbf{Modular Registry Pattern}: Allows fair comparison of arbitrary neural network architectures without modifying the training loop, promoting reproducible benchmarking.
    
    \item \textbf{Physics-Aware Metrics}: Automatically reports errors in physical units (mm, m/s, GPa), enabling direct assessment against engineering tolerances.
    
    \item \textbf{Production-Ready Engineering}: Mixed-precision training, PyTorch 2.x compilation, and Weights \& Biases integration provide the features essential for modern deep learning workflows.
\end{enumerate}

The experimental validation demonstrates that WaveDL achieves sub-0.1~mm thickness accuracy and sub-5~m/s velocity accuracy on Lamb wave inversion, with inference speeds exceeding 2000$\times$ faster than traditional numerical solvers. This performance enables real-time industrial inspection at rates previously unattainable.

WaveDL is actively maintained and available at \url{https://github.com/ductho-le/WaveDL} under the permissive MIT license. We welcome contributions from the community, including new model architectures, additional utilities, and documentation improvements. By providing a common, validated infrastructure for guided wave deep learning, we hope to accelerate progress in this rapidly evolving field.

%% ============================================================================
%% ACKNOWLEDGMENTS
%% ============================================================================
\section*{Acknowledgments}

Ductho Le acknowledges the Natural Sciences and Engineering Research Council of Canada (NSERC) and Alberta Innovates for supporting this research through a research assistantship and graduate doctoral fellowship, respectively. This research was enabled in part by computational resources provided by Compute Ontario, Calcul Qu\'{e}bec, and the Digital Research Alliance of Canada.

%% ============================================================================
%% REFERENCES
%% ============================================================================
\begin{thebibliography}{99}

\bibitem{rose2014}
Rose, J.L. (2014). \textit{Ultrasonic Guided Waves in Solid Media}. Cambridge University Press.

\bibitem{su2009}
Su, Z., \& Ye, L. (2009). \textit{Identification of Damage Using Lamb Waves: From Fundamentals to Applications}. Springer.

\bibitem{balasubramaniam2008}
Balasubramaniam, K., \& Krishnamurthy, C.V. (2008). Inverse characterization of composites from limited ultrasonic data using optimization methods. \textit{Review of Quantitative Nondestructive Evaluation}, 27, 147--154.

\bibitem{rautela2021}
Rautela, M., \& Gopalakrishnan, S. (2021). Deep learning for structural health monitoring: A review of Lamb wave-based damage detection. \textit{Ultrasonics}, 116, 106496.

\bibitem{miorelli2021}
Miorelli, R., Artusi, X., Reboud, C., Theodoulidis, T., \& Poulakis, N. (2021). Supervised deep learning for ultrasonic crack characterization using numerical simulations. \textit{NDT \& E International}, 119, 102405.

\bibitem{yang2022}
Yang, H., Liu, J., Wang, Y., \& Liu, J. (2022). A convolutional neural network approach for inverse acoustic characterization of guided wave measurement. \textit{Mechanical Systems and Signal Processing}, 169, 108759.

\bibitem{zhang2023}
Zhang, Y., Wang, X., Yang, Z., \& Liu, Y. (2023). Physics-informed neural networks for Lamb wave-based damage identification in composite laminates. \textit{Engineering Applications of Artificial Intelligence}, 117, 105564.

\bibitem{raissi2019}
Raissi, M., Perdikaris, P., \& Karniadakis, G.E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. \textit{Journal of Computational Physics}, 378, 686--707.

\bibitem{falcon2019}
Falcon, W., \& The PyTorch Lightning team. (2019). \textit{PyTorch Lightning}. \url{https://pytorch-lightning.readthedocs.io}

\bibitem{chollet2015}
Chollet, F. (2015). \textit{Keras}. \url{https://keras.io}

\bibitem{monai2020}
MONAI Consortium. (2020). \textit{MONAI: Medical Open Network for AI}. \url{https://monai.io}

\bibitem{wolf2020}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., et al. (2020). Transformers: State-of-the-art natural language processing. \textit{Proceedings of EMNLP}, 38--45.

\bibitem{gugger2022}
Gugger, S., Debut, L., Wolf, T., Schmid, P., Mueller, Z., \& Manber, M. (2022). \textit{Accelerate}. \url{https://huggingface.co/docs/accelerate}

\bibitem{li2020}
Li, S., Zhao, Y., Varma, R., Salpekar, O., Noordhuis, P., Li, T., et al. (2020). PyTorch distributed: Experiences on accelerating data parallel training. \textit{Proceedings of the VLDB Endowment}, 13(12), 3005--3018.

\bibitem{cbam2018}
Woo, S., Park, J., Lee, J.Y., \& Kweon, I.S. (2018). CBAM: Convolutional block attention module. \textit{Proceedings of the European Conference on Computer Vision (ECCV)}, 3--19.

\bibitem{paszke2019}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., et al. (2019). PyTorch: An imperative style, high-performance deep learning library. \textit{Advances in Neural Information Processing Systems}, 32.

\bibitem{wandb2020}
Biewald, L. (2020). \textit{Experiment Tracking with Weights and Biases}. \url{https://www.wandb.com}

\bibitem{wu2018}
Wu, Y., \& He, K. (2018). Group normalization. \textit{Proceedings of the European Conference on Computer Vision (ECCV)}, 3--19.

\bibitem{resnet2016}
He, K., Zhang, X., Ren, S., \& Sun, J. (2016). Deep residual learning for image recognition. \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 770--778.

\bibitem{efficientnet2019}
Tan, M., \& Le, Q.V. (2019). EfficientNet: Rethinking model scaling for convolutional neural networks. \textit{International Conference on Machine Learning}, 6105--6114.

\bibitem{vit2021}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., et al. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. \textit{International Conference on Learning Representations}.

\end{thebibliography}

%% ============================================================================
%% APPENDIX A
%% ============================================================================
\appendix
\section{Complete Command Line Reference}
\label{app:cli}

\begin{table*}[htbp]
\centering
\small
\caption{Command line arguments}
\label{tab:cli}
\begin{tabular}{llll}
\toprule
\textbf{Argument} & \textbf{Type} & \textbf{Default} & \textbf{Description} \\
\midrule
\texttt{--model} & str & \texttt{ratenet} & Registered model architecture \\
\texttt{--list\_models} & flag & -- & Print available models and exit \\
\texttt{--batch\_size} & int & 128 & Per-GPU batch size \\
\texttt{--lr} & float & 0.001 & Initial learning rate \\
\texttt{--epochs} & int & 1000 & Maximum epochs \\
\texttt{--patience} & int & 20 & Early stopping patience \\
\texttt{--weight\_decay} & float & 0.0001 & AdamW weight decay \\
\texttt{--grad\_clip} & float & 1.0 & Maximum gradient norm \\
\texttt{--data\_path} & str & \texttt{train\_data.npz} & Training data path \\
\texttt{--workers} & int & 8 & DataLoader workers \\
\texttt{--seed} & int & 2025 & Random seed \\
\texttt{--resume} & str & None & Checkpoint to resume \\
\texttt{--save\_every} & int & 10 & Checkpoint frequency \\
\texttt{--output\_dir} & str & \texttt{.} & Output directory \\
\texttt{--compile} & flag & -- & Enable torch.compile \\
\texttt{--precision} & str & \texttt{bf16} & Mixed precision mode \\
\texttt{--wandb} & flag & -- & Enable W\&B tracking \\
\texttt{--project\_name} & str & \texttt{DL-Training} & W\&B project name \\
\texttt{--run\_name} & str & None & W\&B run name \\
\bottomrule
\end{tabular}
\end{table*}

%% ============================================================================
%% APPENDIX B
%% ============================================================================
\section{File Format Specification}
\label{app:formats}

\subsection{Input Data (NPZ)}

\begin{lstlisting}[caption={Required keys in train\_data.npz}]
'input_train': numpy.ndarray
    # Shape: (num_samples, height, width)
    # Dtype: float32 recommended
    # Content: 2D representations of guided wave signals
    #   - Dispersion curves (frequency-wavenumber)
    #   - Spectrograms (time-frequency)
    #   - B-scans (position-time)
    #   - Other 2D transforms

'output_train': numpy.ndarray
    # Shape: (num_samples, num_targets)
    # Dtype: float32 recommended
    # Content: Material properties to predict
    #   - Thickness (mm)
    #   - Wave velocity (m/s)
    #   - Elastic moduli (GPa)
    #   - Density (kg/m^3)
    #   - Custom parameters
\end{lstlisting}

\subsection{Checkpoint Structure}

\begin{verbatim}
best_checkpoint/
|-- model.safetensors      # Model weights (via Accelerate)
|-- optimizer.bin          # Optimizer state
|-- scheduler.bin          # LR scheduler state
|-- random_states.pkl      # RNG states for reproducibility
+-- training_meta.pkl      # Custom metadata:
                           #   - epoch: int
                           #   - best_val_loss: float
                           #   - patience_ctr: int
\end{verbatim}

\subsection{Output Files}

\begin{verbatim}
output_directory/
|-- best_checkpoint/       # Best model checkpoint
|-- epoch_10_checkpoint/   # Periodic checkpoint (if --save_every=10)
|-- epoch_20_checkpoint/
|-- ...
|-- best_model_weights.pth # Standalone weights file
|-- training_history.csv   # Epoch-by-epoch metrics
|-- train_data_cache.dat   # Memory-mapped input cache
|-- scaler.pkl             # Fitted StandardScaler
+-- data_metadata.pkl      # Shape and dimension info
\end{verbatim}

%% ============================================================================
%% SOFTWARE AVAILABILITY
%% ============================================================================
\section*{Software Availability}

The WaveDL framework is freely available under the MIT License. Source code, documentation, and usage examples can be obtained from the GitHub repository: \url{https://github.com/ductho-le/WaveDL}. The software has been tested on Linux systems (Ubuntu 20.04, CentOS 7) with Python 3.11+ and PyTorch 2.0+.
