%% WaveDL: A Scalable Deep Learning Framework for Guided Wave Inversion
%% Target journal: Computer Physics Communications
%% LaTeX document class: cas-dc (Elsevier CAS Double Column)

\documentclass[a4paper,fleqn]{cas-dc}

%% Bibliography
\usepackage[numbers]{natbib}

%% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{url}
\usepackage{float}

%% Code listing style
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Python
}

\lstdefinestyle{pseudocode}{
    basicstyle=\ttfamily\small,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=none,
    showspaces=false,
    tabsize=2,
}

\lstset{style=pythonstyle}

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}

%% Short title and authors for running headers
\shorttitle{WaveDL: Deep Learning Framework for Guided Wave Inversion}
\shortauthors{D. Le}

%% =============================================================================
%% TITLE
%% =============================================================================
\title[mode=title]{WaveDL: A Scalable Deep Learning Framework for Guided Wave Inversion on High-Performance Computing Clusters}

%% =============================================================================
%% AUTHORS
%% =============================================================================
\author[1]{Ductho Le}[type=author]
\cormark[1]
\ead{ductho.le@outlook.com}

\credit{Conceptualization, Methodology, Software, Validation, Writing - Original Draft, Writing - Review \& Editing}

\cortext[1]{Corresponding author}

%% =============================================================================
%% AFFILIATIONS
%% =============================================================================
\affiliation[1]{organization={University of Alberta},
                city={Edmonton},
                state={Alberta},
                country={Canada}}

%% =============================================================================
%% ABSTRACT
%% =============================================================================
\begin{abstract}
Ultrasonic wave inspection is a powerful technique for non-destructive evaluation (NDE) of structures such as pipelines, aircraft components, and composite materials, yet a central challenge lies in solving the inverse problem of inferring material and structural properties (e.g., thickness, elastic moduli, crack location) from measured wavefields. Traditional approaches rely on iterative optimization with physic-based forward simulations, which are computationally intensive and difficult to deploy in real time. Deep learning provides a promising alternative by learning direct mappings from signal representations to physical parameters; however, large-scale wave-learning problems introduce substantial engineering challenges---training datasets often exceed available system memory and require specialized out-of-core pipelines, multi-GPU training demands careful synchronization to maintain numerical stability and efficiency, and the field lacks standardized frameworks for reproducibility and fair benchmarking across architectures and datasets. To address these limitations, this paper introduces WaveDL, an open-source Python framework that provides a memory-efficient data pipeline using memory-mapped files, an architecture-agnostic interface for integrating and comparing neural networks, robust distributed training with synchronized multi-GPU execution, and automated ONNX export for deployment into industrial ecosystems such as LabVIEW, MATLAB, and C++. Built on PyTorch and Hugging Face Accelerate, WaveDL supports mixed-precision training, experiment tracking with Weights \& Biases, and includes over 200 unit tests to ensure reliability. WaveDL is released under the MIT license at \url{https://github.com/ductho-le/WaveDL}.
\end{abstract}

%% =============================================================================
%% KEYWORDS
%% =============================================================================
\begin{keywords}
deep learning framework \sep guided wave inversion \sep distributed training \sep high-performance computing \sep memory-mapped data \sep multi-GPU synchronization \sep neural network regression
\end{keywords}

\maketitle

%% ============================================================================
%% PROGRAM SUMMARY
%% ============================================================================
\section*{Program Summary}

\noindent\textbf{Program title:} WaveDL (Wave Deep Learning)

\noindent\textbf{Licensing provisions:} MIT License

\noindent\textbf{Programming language:} Python 3.11+

\noindent\textbf{Repository:} \url{https://github.com/ductho-le/WaveDL}

\noindent\textbf{Nature of problem:}
Ultrasonic guided wave inversion requires determining material properties (thickness, elastic moduli, density) from measured wave signals. Traditional iterative optimization methods coupled with physics-based forward models are computationally expensive. Deep learning enables direct mapping from signals to properties with millisecond inference time. However, training neural networks on the large-scale simulation datasets typical in this domain presents significant computational challenges: datasets often exceed available RAM, multi-GPU training requires careful synchronization to avoid deadlocks, and there is a lack of standardized frameworks for reproducibility and fair benchmarking across architectures.

\noindent\textbf{Solution method:}
WaveDL implements a zero-copy memory-mapped data pipeline using NumPy memmap, enabling random-access training on datasets larger than available RAM without performance degradation. The framework automatically detects and handles 1D, 2D, and 3D input data, adding channel dimensions as needed. A decorator-based model registry pattern allows researchers to integrate arbitrary neural network architectures without modifying the training infrastructure. Distributed Data Parallel (DDP) utilities provide synchronized early stopping across multiple GPUs. Additionally, WaveDL includes automated ONNX model export to facilitate deployment into industrial ecosystems such as LabVIEW, MATLAB, and C++.

\noindent\textbf{External routines/libraries:}
PyTorch ($\geq$2.0), HuggingFace Accelerate ($\geq$0.20), NumPy ($\geq$1.24), SciPy ($\geq$1.10), scikit-learn ($\geq$1.2), pandas ($\geq$2.0), matplotlib ($\geq$3.7), tqdm ($\geq$4.65), Weights \& Biases ($\geq$0.15, optional)

\noindent\textbf{Restrictions:}
The framework is designed for multi-output regression tasks on 2D input representations. Classification tasks or 1D/3D inputs require modifications to the data pipeline and loss functions. Training requires NVIDIA GPU with CUDA support; CPU-only inference is supported.

%% ============================================================================
%% INTRODUCTION
%% ============================================================================
\section{Introduction}
\label{sec:introduction}

\subsection{Background: Ultrasonic Guided Waves in Non-Destructive Evaluation}

Ultrasonic guided waves are elastic wave modes that propagate within bounded media such as plates, pipes, shells, and layered composites. Unlike conventional bulk ultrasonic waves that travel through the material thickness, guided waves can propagate over extended distances (tens to hundreds of meters) while remaining sensitive to structural features throughout the waveguide cross-section~\cite{rose2014,su2009}. This unique property makes them invaluable for the rapid inspection of large-scale infrastructure including pipelines, railway tracks, aircraft fuselages, and wind turbine blades.

The physics of guided wave propagation is governed by the elastic wave equation in bounded media. For an isotropic plate of thickness $h$, the displacement field $\mathbf{u}(\mathbf{x}, t)$ satisfies:
\begin{equation}
\rho \frac{\partial^2 \mathbf{u}}{\partial t^2} = (\lambda + \mu) \nabla(\nabla \cdot \mathbf{u}) + \mu \nabla^2 \mathbf{u}
\label{eq:elastic_wave}
\end{equation}
where $\rho$ is the mass density, and $\lambda$, $\mu$ are the Lam\'{e} constants related to the longitudinal ($c_L$) and shear ($c_T$) wave velocities. The boundary conditions imposed by the traction-free surfaces give rise to a family of discrete wave modes (Lamb waves in plates, torsional and longitudinal modes in pipes) whose phase velocity $c_p$ depends on the frequency-thickness product $fh$. This relationship, known as the \textit{dispersion curve}, encodes the material's mechanical properties and geometry.

% Figure 1: Lamb wave modes
\begin{figure*}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{images/fig11_guided_waves.png}
\caption{Lamb wave modes in an isotropic plate. Symmetric modes (S0, S1) exhibit in-plane particle motion while antisymmetric modes (A0, A1) show bending-like displacement. The dispersion relationship $c_p(f \cdot h)$ encodes material properties.}
\label{fig:guided_waves}
\end{figure*}

\subsection{The Inverse Characterization Problem}

The fundamental challenge in guided wave-based material characterization is the \textit{inverse problem}: given an observed set of dispersion data $\mathbf{x}$ (e.g., a frequency-wavenumber spectrum or time-frequency representation), determine the material property vector $\mathbf{p} = [h, c_L, c_T, \rho, ...]^T$ that produced it.

Formally, let $\mathcal{G}: \mathcal{P} \to \mathcal{X}$ denote the \textit{forward operator} that maps material properties $\mathbf{p} \in \mathcal{P}$ to observable dispersion data $\mathbf{x} \in \mathcal{X}$. The inverse problem seeks the operator $\mathcal{G}^{-1}$ such that:
\begin{equation}
\mathbf{p} = \mathcal{G}^{-1}(\mathbf{x})
\label{eq:inverse_problem}
\end{equation}

This problem is well-posed in the Hadamard sense only under specific conditions~\cite{balasubramaniam2008}. In practice, noise, incomplete mode identification, and model-data discrepancies introduce ill-posedness, requiring regularization techniques.

Traditional solution approaches fall into two categories:
\begin{enumerate}
    \item \textbf{Iterative Optimization}: Given an initial guess $\mathbf{p}_0$, iteratively refine $\mathbf{p}$ to minimize the objective:
    \begin{equation}
    \mathbf{p}^* = \arg\min_{\mathbf{p}} \|\mathcal{G}(\mathbf{p}) - \mathbf{x}_{obs}\|^2 + \lambda R(\mathbf{p})
    \label{eq:optimization}
    \end{equation}
    where $R(\mathbf{p})$ is a regularization term. Methods include Levenberg-Marquardt, genetic algorithms, and particle swarm optimization. These require repeated forward model evaluations (seconds to minutes each for complex layered media), making real-time inversion impractical.
    
    \item \textbf{Look-up Tables}: Pre-compute $\mathcal{G}(\mathbf{p}_i)$ for a dense grid $\{\mathbf{p}_i\}$ in parameter space and perform nearest-neighbor matching. While fast at inference, this approach scales poorly with the number of parameters (curse of dimensionality) and provides only discrete solutions.
\end{enumerate}

% Figure 2: Forward and inverse problems
\begin{figure*}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{images/fig10_inverse_problem.png}
\caption{Forward and inverse problems in guided wave characterization. The forward problem maps material properties to dispersion curves via physics-based simulation. Deep learning directly approximates the inverse mapping, bypassing iterative optimization.}
\label{fig:inverse_problem}
\end{figure*}

These limitations have catalyzed the adoption of data-driven strategies, with deep learning emerging as a particularly promising alternative. Instead of repeatedly invoking a forward model or exhaustively sampling the parameter space, a neural network learns the inverse mapping directly from representative example pairs, enabling near-instantaneous inference once trained. Successful demonstrations span multiple wave-based domains, including seismic inversion~\cite{raissi2019}, electromagnetic scattering~\cite{yang2022}, and ultrasonic NDE problems such as defect identification and elastic property estimation~\cite{rautela2021,miorelli2021,zhang2023}.

However, several practical obstacles still hinder the reliable application of deep learning in ultrasonic inspection. Large-scale datasets---often hundreds of gigabytes for realistic resolutions---routinely exceed the memory and storage capacity of typical compute environments, necessitating specialized out-of-core pipelines. Multi-GPU training further introduces coordination complexities involving synchronized early stopping, consistent metric aggregation, and safe checkpointing. Additionally, while machine learning frameworks commonly report normalized losses, NDE practitioners require prediction errors expressed in physically interpretable units (e.g., millimeters, meters per second, or gigapascals) to facilitate meaningful model evaluation and deployment.

%% ============================================================================
%% RELATED WORK
%% ============================================================================
\section{Related Work}
\label{sec:related_work}

\subsection{Deep Learning for Guided Wave Applications}

The application of deep learning to guided wave problems has accelerated rapidly since 2018. Key developments include:

\textbf{Damage Detection and Classification}: Rautela and Gopalakrishnan~\cite{rautela2021} provided a comprehensive review of Lamb wave-based damage detection using CNNs and recurrent networks. Their analysis highlighted that 2D time-frequency representations (spectrograms, scalograms) consistently outperform raw 1D waveforms as network inputs due to the explicit encoding of dispersive behavior.

\textbf{Inverse Characterization}: Miorelli et al.~\cite{miorelli2021} demonstrated supervised deep learning for ultrasonic crack characterization using synthetically generated training data from finite element simulations. They achieved sizing accuracy comparable to expert human inspectors while reducing inference time from minutes to milliseconds.

\textbf{Physics-Informed Approaches}: Zhang et al.~\cite{zhang2023} integrated physics-based loss terms derived from Lamb wave dispersion equations into neural network training, improving generalization to out-of-distribution samples. Raissi et al.~\cite{raissi2019} pioneered Physics-Informed Neural Networks (PINNs) as a general framework for solving forward and inverse problems involving PDEs.

Despite these advances, most published studies rely on custom, single-use training scripts that lack generalizability, documentation, and community support. There is a clear need for standardized, validated software infrastructure.

\subsection{Existing Deep Learning Frameworks}

Several general-purpose frameworks exist for neural network training:

\textbf{PyTorch Lightning}~\cite{falcon2019} abstracts boilerplate code and provides distributed training support, but imposes an opinionated project structure that may conflict with research workflows. It does not provide domain-specific utilities for physics applications.

\textbf{Keras/TensorFlow}~\cite{chollet2015} offers high-level APIs but lacks native support for memory-mapped data loading and has weaker multi-GPU debugging tools compared to PyTorch.

\textbf{MONAI}~\cite{monai2020} is a domain-specific framework for medical imaging that includes specialized data transforms, network architectures, and loss functions. While highly successful in its niche, it is not applicable to guided wave problems.

\textbf{Hugging Face Accelerate}~\cite{gugger2022} provides lightweight distributed training utilities without imposing structural constraints. WaveDL builds upon Accelerate while adding physics-aware features specific to wave inversion.

% Figure 3: Framework comparison
\begin{figure*}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{images/fig12_framework_comparison.png}
\caption{Framework feature comparison. WaveDL is the only framework combining memory-mapped data loading, DDP synchronization utilities, physics-aware metrics, and domain-specific design in a single package.}
\label{fig:framework_comparison}
\end{figure*}

To date, no framework has been designed specifically for the computational requirements of guided wave inverse problems: out-of-core data loading, physics-aware metrics, and HPC-ready distributed synchronization.

\subsection{Contributions of This Work}

WaveDL addresses the identified gaps through five principal contributions:
\begin{enumerate}
    \item \textbf{Zero-Copy Memory-Mapped Data Pipeline}: A thread-safe data loading system that enables training on datasets exceeding available RAM by leveraging OS virtual memory management. Unlike generic PyTorch \texttt{IterableDataset} approaches, our implementation correctly handles multiprocessing worker initialization to prevent file descriptor sharing.
    
    \item \textbf{Decorator-Based Model Registry}: A compile-time dependency injection pattern that allows researchers to register arbitrary neural network architectures without modifying the training loop. This promotes fair comparison of different models under identical training conditions.
    
    \item \textbf{DDP-Safe Synchronization Primitives}: Utility functions that broadcast early stopping decisions, aggregate metrics across processes, and coordinate checkpoint saving---resolving the deadlock and race condition issues endemic to na\"{i}ve distributed implementations.
    
    \item \textbf{Physics-Aware Metric Tracking}: Automatic computation of Mean Absolute Error in original physical units (mm, m/s, GPa) by applying inverse standardization transformations, enabling direct assessment against engineering tolerances.
    
    \item \textbf{Production-Ready Engineering}: Mixed-precision (BF16/FP16) support, PyTorch 2.x compilation compatibility, Weights \& Biases experiment tracking, and robust checkpoint/resume functionality---features essential for HPC deployment but often missing from research code.
\end{enumerate}

The remainder of this paper is organized as follows: Section~\ref{sec:methodology} details the computational methodology, including the mathematical formulation of the data pipeline and distributed synchronization algorithms. Section~\ref{sec:architectures} describes the reference architectures and extensibility mechanisms. Section~\ref{sec:experiments} presents comprehensive experimental validation, and Section~\ref{sec:discussion} concludes with a discussion of the software's impact and future directions.

%% Include the rest of the document from remaining parts
\input{wavedl_paper_part2}
\input{wavedl_paper_part3}
\input{wavedl_paper_part4}

\end{document}
