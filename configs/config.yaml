# WaveDL Configuration Template
# ==============================
# This template includes ALL available options with their defaults.
# Copy and modify for your experiments. CLI arguments override any values.
#
# Usage:
#   wavedl-train --config configs/config.yaml
#   wavedl-train --config configs/config.yaml --lr 5e-4

# =============================================================================
# MODEL
# =============================================================================
model: cnn                    # 69 architectures available:
                              #
                              # ── Classic CNNs ──
                              # CNN:       cnn
                              # ResNet:    resnet18, resnet34, resnet50,
                              #            resnet18_pretrained, resnet50_pretrained
                              # DenseNet:  densenet121, densenet169, densenet121_pretrained
                              #
                              # ── Efficient/Mobile CNNs ──
                              # MobileNetV3: mobilenet_v3_small, mobilenet_v3_large
                              # EfficientNet: efficientnet_b0, efficientnet_b1, efficientnet_b2
                              # EfficientNetV2: efficientnet_v2_s, efficientnet_v2_m, efficientnet_v2_l
                              # RegNet:    regnet_y_400mf, regnet_y_800mf, regnet_y_1_6gf,
                              #            regnet_y_3_2gf, regnet_y_8gf
                              #
                              # ── Modern CNNs ──
                              # ConvNeXt:  convnext_tiny, convnext_small, convnext_base,
                              #            convnext_tiny_pretrained
                              # ConvNeXt V2: convnext_v2_tiny, convnext_v2_small, convnext_v2_base,
                              #              convnext_v2_tiny_pretrained
                              # UniRepLKNet: unireplknet_tiny, unireplknet_small, unireplknet_base
                              #
                              # ── Vision Transformers ──
                              # ViT:       vit_tiny, vit_small, vit_base
                              # Swin:      swin_t, swin_s, swin_b
                              # MaxViT:    maxvit_tiny, maxvit_small, maxvit_base
                              #
                              # ── Hybrid CNN-Transformer ──
                              # FastViT:   fastvit_t8, fastvit_t12, fastvit_s12, fastvit_sa12
                              # CAFormer:  caformer_s18, caformer_s36, caformer_m36, poolformer_s12
                              # EfficientViT: efficientvit_m0, efficientvit_m1, efficientvit_m2,
                              #               efficientvit_b0, efficientvit_b1, efficientvit_b2,
                              #               efficientvit_b3, efficientvit_l1, efficientvit_l2
                              #
                              # ── State Space Models ──
                              # Mamba:     mamba_1d
                              # Vision Mamba: vim_tiny, vim_small, vim_base
                              #
                              # ── Specialized Architectures ──
                              # TCN:       tcn_small, tcn, tcn_large
                              # ResNet3D:  resnet3d_18, mc3_18
                              # U-Net:     unet_regression

pretrained: true              # Use ImageNet pretrained weights (for supported models)
                              # Set to false to train from scratch

# =============================================================================
# TRAINING HYPERPARAMETERS
# =============================================================================
batch_size: 128               # Per-GPU batch size
lr: 0.001                     # Learning rate (higher for OneCycleLR, e.g., 0.01)
epochs: 1000                  # Maximum training epochs
patience: 20                  # Early stopping patience (epochs without improvement)
weight_decay: 0.0001          # L2 regularization (AdamW)
grad_clip: 1.0                # Gradient clipping max norm

# =============================================================================
# LOSS FUNCTION
# =============================================================================
loss: mse                     # Options: mse, mae, huber, smooth_l1, log_cosh, weighted_mse
# huber_delta: 1.0            # Delta for Huber loss (only if loss: huber)
# loss_weights: "1.0,2.0,1.0" # Comma-separated weights for weighted_mse

# =============================================================================
# OPTIMIZER
# =============================================================================
optimizer: adamw              # Options: adamw, adam, sgd, nadam, radam, rmsprop
# momentum: 0.9               # Momentum for SGD/RMSprop
# nesterov: false             # Use Nesterov momentum (SGD only)
# betas: "0.9,0.999"          # Betas for Adam variants (comma-separated)

# =============================================================================
# LEARNING RATE SCHEDULER
# =============================================================================
scheduler: plateau            # Options: plateau, cosine, cosine_restarts, onecycle,
                              #          step, multistep, exponential, linear_warmup
# scheduler_factor: 0.5       # LR reduction factor (for plateau/step)
# scheduler_patience: 10      # Epochs to wait before reducing (for plateau)
# min_lr: 0.000001            # Minimum learning rate
# warmup_epochs: 5            # Warmup epochs (for linear_warmup)
# step_size: 30               # Step size for StepLR
# milestones: "30,60,90"      # Comma-separated epochs for MultiStepLR

# =============================================================================
# CROSS-VALIDATION (Optional)
# =============================================================================
cv: 0                         # Number of folds (0 = disabled, e.g., 5 for 5-fold CV)
cv_stratify: false            # Stratified splits based on output values
cv_bins: 10                   # Number of bins for stratification

# =============================================================================
# DATA & OUTPUT
# =============================================================================
# data_path: train_data.npz   # Required - path to training data
# output_dir: ./experiments   # Output directory for checkpoints/logs
# resume: null                # Path to checkpoint to resume from
# save_every: 50              # Save checkpoint every N epochs (0 = disabled)
# fresh: false                # Start fresh, ignore existing checkpoints
workers: -1                   # DataLoader workers (-1 = auto-detect)
seed: 2025                    # Random seed for reproducibility
deterministic: false          # Deterministic mode for exact reproducibility (slower)
cache_validate: sha256        # Cache validation: sha256 (full), fast (partial), size (quick)
# single_channel: false       # Confirm data is single-channel (for shallow 3D volumes)
# import_modules: []          # Python modules/files to import for custom models
                              # Supports: module names (my_model) or file paths (./my_model.py)

# =============================================================================
# PERFORMANCE OPTIMIZATION
# =============================================================================
precision: bf16               # Options: bf16, fp16, no (for fp32)
compile: false                # torch.compile() for faster training (PyTorch 2.0+)

# =============================================================================
# LOGGING & MONITORING
# =============================================================================
# wandb: false                # Enable Weights & Biases logging
# wandb_watch: false          # Enable W&B gradient watching (adds overhead, useful for debugging)
# project_name: DL-Training   # W&B project name
# run_name: null              # W&B run name (auto-generated if null)

# =============================================================================
# PHYSICAL CONSTRAINTS (Optional)
# =============================================================================
# Enforce physical laws during training via penalty terms:
#   Total Loss = Data Loss + weight × penalty(violation)
#
# constraint: []              # Expression(s): ["y0 > 0", "y0 - y1*y2"]
# constraint_file: null       # Python file with constraint(pred, inputs) function
# constraint_weight: [0.1]    # Penalty weight(s) per constraint
# constraint_reduction: mse   # Reduction: mse (squared) or mae (linear)
#
# Expression syntax:
#   Variables: y0, y1, ... (outputs), x[0], x[1], ... (inputs)
#   Aggregates: x_mean, x_sum, x_max, x_min, x_std
#   Operators: +, -, *, /, **, >, <, >=, <=, ==
#   Functions: sin, cos, exp, log, sqrt, sigmoid, softplus, tanh, relu, abs
